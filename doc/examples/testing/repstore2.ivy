#lang ivy1.6

# Replicating key-value store
# ===========================
#
# Now we will consider a distributed implementation of the key/value
# store.  This version is replicating. We will have two server
# processes, a *primary* and a *secondary*. Each keeps a complete
# replica of the store.  All write requsts go first to the primary and
# then are passed along to the secondary. Read requests, on the other
# hand, may be sent to either server. We will assume that the channel
# between the primary and secondary is ordered.

# Now we face a question: since transactions happen in parallel, and
# no one server sees all the transactions, how do we define commit
# points? At first we might imagine committing the write transactions
# when they execute on the primary, and the reads transactions on
# whichever server they arrive at. Clearly, this won't work, however.
# Suppose a write has been committed on the primary and is on its way
# to the secondary. If the secondary now commits a read for the same
# key, it will return a stale value for that read. The situation is
# reversed if we commit writes when they execute at the secondary,
# since a read on the primary could returna "future" value.
#
# In fact, there is no easy way out of this dilemma. If we don't add
# some additional concurrency control, we will necessarily violate
# linearizability. To fix the problem, we will add a reverse channel
# from the secondary to the primary to acknowldege when writes have
# been committed on the secondary. If there is a pending,
# unacknowledged write on the primary, it must wait to serve any reads
# for that key until write is acknowledged.
#
# The key property we must then maintain on the channel is that writes
# occur *in the same order* in which they are executed on the primary.
# This does not mean that forwarding of a write cannot occur before
# the actual execution on the primrary. It just means the order must
# be preserved. We will call this the *serialization* order. Thus,
# writes are serialized on the primary (meaning their order is
# determined) but *committed* on the secondary).

# Using this specification, we can test the primary and secondary
# servers in isolation, and if they are correct according to their
# assume.guarantee specifications, we can conclude that the together
# provide a correct service according to our reference object,

# Basic data types
# ----------------

# As with he simple one-process server (see
# [repstore1.ivy](repstore1.html)), we begin by declaring the data
# types needed to represent transactions and the messages that carry
# them.

# The key and value types are uninterpreted for the moment.


type key
type value
ghost type txid

type request_kind = {write,read}

object request_body = {
    type t = struct {
	knd : request_kind,
	ky : key,
	vl : value
    }
}

object request = {
    type t = struct {
	tx : txid,
	bd : request_body.t
    }
}

object response_body = {
    type t = struct {
	vl : value
    }
}

object response = {
    type t = struct {
	tx : txid,
	bd : response_body.t
    }
}

# The definition of a replica is also the same as before.

module replica = {
    function store(K:key) : value
    after init {
	store(K) := 0
    }
    action exec(inp : request_body.t) returns (out:response_body.t) = {
	if inp.knd = write {
	    store(inp.ky) := inp.vl;
	}
	else if inp.knd = read {
	    out.vl := store(inp.ky);
	}
    }
}

# The first difference is that we now have two servers, so the
# enumerated type `dest.t` has two values, one for the primary and one
# for the secondary.

object dest = {
    ghost type t = {prim,sec}
}

# Reference object
# ----------------

# Our reference model is the same as before.

object ref = {
    action create(inp : request_body.t, dst:dest.t) returns (tx : txid)
    action serialize(tx : txid)
    action commit(tx : txid,dst:dest.t)
    action eval(tx : txid) returns (res : response_body.t)

    instance rep : replica
    
    individual next : txid
    function txs(X:txid) : request_body.t
    function txres(X:txid) : response_body.t
    relation committed(X:txid)

    after init {
	next := 0;
	committed(X) := false;
    }

    implement create {
	tx := next;
	txs(tx) := inp;
	next := next + 1;
    }

    implement commit {
	assert 0 <= tx & tx < next;
	assert ~committed(tx);
	txres(tx) := rep.exec(txs(tx));
	committed(tx) := true;
    }
    delegate commit

    implement eval {
	assert committed(tx);
	res := txres(tx);
    }
    delegate eval

    interpret txid -> int
}



# This time, however, we'll add another reference object to keep track
# of the serialization order, so we can specify the interface between
# primary and secondary. It has one action `serialize` that indicates when
# a write transaction is serialized.

object serializer = {
    
    action serialize(tx : txid)

    # The serializer object keeps track of which transactions have
    # been serialized.

    relation serialized(X:txid)

    after init {
	serialized(X) := false;
    }

    # To serialzie a write, we must guarantee that transaction exists,
    # and that it has not already been serialized. Further, we can
    # only serialize write transactions.

    implement serialize {
	assert 0 <= tx & tx < next;
	assert ~serialized(tx);
	assert txs(tx).knd = write;
	serialized(tx) := true;
    }
    delegate serialize

    # Further,we specify that a write transaction cannot be committed
    # until it is serialized.

    before ref.commit {
	assert txs(tx).knd = write -> serialized(tx);
    }
}

# The implementation
# ------------------

# Now we are ready to define our system implemention, consisting of
# client endpoints, the primary server and the secondary server.

# Notice we include the `tcp` module here, since we will need it for
# the ordered channels between the servers.

include tcp
include udp

# Again, we have an uninterpreted type of client ids, and a request
# message structure that encapsulates the request with its client id
# for routing the response.

type client_id

type req_msg = struct {
    cid : client_id,
    req : request.t
}

# The client endpoint is the same as before.

module client(cid,prim_chan,sec_chan,cl_chans) = {
    
    action client_request(req : request_body.t, the_dst: dest.t)

    implement client_request {
	local m : req_msg {
	    m.cid := cid;
	    m.req.tx := ref.create(req,the_dst);
	    m.req.bd := req;
	    if the_dst = dest.prim {
	        call prim_chan.send(m);
	    } else {
	        call sec_chan.send(m);
	    }
	}
    }
}

# The primary server module now has two additional parameters, for the
# forward channel (forwarding writes to the secondary) and the reverse
# channel (returning acks).

module primary_node(port, fwd_chan, rev_chan, cl_chans) = {
    instance rep : replica

    # Again, we have an endpoint for receiving requests from clients.

    instance req_chan : nondup_endpoint(port,req_msg)

    # We have to remember how many pending writes each key has. We use
    # a map from keys to counters for this.

    instance counter : unbounded_sequence
    function pending(K:key) : counter.t

    # Initially, all the counters are zero.

    after init {
	pending(K) := 0;
    }
	
    # When receiving a request message from a client, the primary
    # node must first check whether it is a read or a write. In the case
    # of a read, we check if the key has a pending write. If not, we commit
    # the read, execute it, and return the response to the client. If there is
    # a pending write, we postpone the read by forwarding it to ourself. 
    # 

    implement req_chan.recv(inp : req_msg) {
	var rr := inp.req.bd;
        if rr.knd = read {
	    if pending(rr.ky) = 0 {
		call ref.commit(inp.req.tx,dest.prim);
		var res : response.t;
		res.tx := inp.req.tx;
		res.bd := rep.exec(rr);
		call cl_chans(inp.cid).send(res)
	    } else {
                call req_chan.send(inp);  # if cannot execute, recirculate
	    }
	} else if rr.knd = write {
	    call ref.serialize(inp.req.tx);           # this is ghost!
	    call fwd_chan.send(inp);
	    pending(rr.ky) := pending(rr.ky).next;
	    var res := rep.exec(inp.req.bd);
	} 	    
    }

    implement rev_chan.recv(inp : req_msg) {
	var rr := inp.req.bd;
	if rr.knd = write {
	    pending(rr.ky) := pending(rr.ky).prev;
	}
    }
}

module secondary_node(port, fwd_chan, rev_chan, cl_chans) = {
    instance rep : replica

    instance req_chan : nondup_endpoint(port,req_msg)
	
    implement req_chan.recv(inp : req_msg) {
	var rr := inp.req.bd;
        if rr.knd = read {
	    call ref.commit(inp.req.tx,dest.sec);
	    var res : response.t;
	    res.tx := inp.req.tx;
	    res.bd := rep.exec(rr);
	    call cl_chans(inp.cid).send(res);
	} 	    
	# ignore writes!
    }

    implement fwd_chan.recv(inp : req_msg) {
	var res : response.t;
	res.tx := inp.req.tx;
	res.bd := rep.exec(inp.req.bd);
	call cl_chans(inp.cid).send(res);
	call rev_chan.send(inp);
    }

}


instance fwd_chan : tcp_channel("localhost:44090",req_msg)
instance rev_chan : tcp_channel("localhost:44091",req_msg)

instance cl_chans : nondup_endpoint_set(client_id,44100,response.t)
instance cl(X:client_id) : client(X,prim.req_chan,sec.req_chan,cl_chans)
instance prim : primary_node(44200,fwd_chan.sndr,rev_chan.rcvr,cl_chans)
instance sec : secondary_node(44201,fwd_chan.rcvr,rev_chan.sndr,cl_chans)

object service_spec = {
    before cl_chans.send(p : client_id, m : response.t) {
	assert m.bd = ref.eval(m.tx);
    }
}

object mid_spec = {
    instance queue : unbounded_queue(txid)

    after ref.serialize(tx:txid) {
	call queue.push(tx);
    }

    before fwd_chan.rcvr.recv(inp : req_msg) {
	assert inp.req.bd = ref.txs(inp.req.tx);
	assert inp.req.tx = queue.pop;
	call ref.commit(inp.req.tx,dest.sec);
    }

    delegate fwd_chan_rcvr_recv[before] -> prim

    relation acked(X:txid)

    after init {
	acked(X) := false;
    }
    
    before rev_chan.sndr.send(inp : req_msg) {
	assert ref.committed(inp.req.tx);
	assert ~acked(inp.req.tx);
	acked(inp.req.tx) := true;
    }
    
    function req_dest(X:txid) : dest.t

    after ref.create(inp : request_body.t, dst:dest.t) returns (tx : txid) {
	req_dest(tx) := dst
    }
	
    # Reads must be commited by the server they are addressed to, while
    # all writes are committedby the secondary

    before ref.commit(tx : txid,dst:dest.t) {
	assert ref.txs(tx).knd = read -> req_dest(tx) = dst;
	assert ref.txs(tx).knd = write -> dst = dest.sec
    }


}

export cl.client_request
import cl_chans.recv

trusted isolate iso_prim = prim with cl,cl_chans,ref,service_spec,mid_spec,fwd_chan,rev_chan
trusted isolate iso_sec = sec with cl,cl_chans,ref,service_spec,mid_spec

interpret txid -> int
interpret key -> strbv[3]
interpret value -> bv[16]
interpret client_id -> bv[1]
